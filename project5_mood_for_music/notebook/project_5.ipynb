{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7b6cd17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7fd1859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          name                        album  \\\n",
      "0                                         1999                         1999   \n",
      "1                                           23                           23   \n",
      "2                                     9 Crimes                            9   \n",
      "3                               99 Luftballons               99 Luftballons   \n",
      "4  A Boy Brushed Red Living In Black And White  They're Only Chasing Safety   \n",
      "\n",
      "           artist                      id release_date  popularity  length  \\\n",
      "0          Prince  2H7PHVdQ3mXqEHXcvclTB0   1982-10-27          68  379266   \n",
      "1  Blonde Redhead  4HIwL9ii9CcXpTOTzMq0MP   2007-04-16          43  318800   \n",
      "2     Damien Rice  5GZEeowhvSieFDiR8fQ2im   2006-11-06          60  217946   \n",
      "3            Nena  6HA97v4wEGQ5TUClRM0XLc   1984-08-21           2  233000   \n",
      "4       Underoath  47IWLfIKOKhFnz1FUEUIkE   2004-01-01          60  268000   \n",
      "\n",
      "   danceability  acousticness  energy  instrumentalness  liveness  valence  \\\n",
      "0         0.866       0.13700   0.730          0.000000    0.0843    0.625   \n",
      "1         0.381       0.01890   0.832          0.196000    0.1530    0.166   \n",
      "2         0.346       0.91300   0.139          0.000077    0.0934    0.116   \n",
      "3         0.466       0.08900   0.438          0.000006    0.1130    0.587   \n",
      "4         0.419       0.00171   0.932          0.000000    0.1370    0.445   \n",
      "\n",
      "   loudness  speechiness    tempo  key  time_signature       mood  \n",
      "0    -8.201       0.0767  118.523    5               4      Happy  \n",
      "1    -5.069       0.0492  120.255    8               4        Sad  \n",
      "2   -15.326       0.0321  136.168    0               4        Sad  \n",
      "3   -12.858       0.0608  193.100    4               4      Happy  \n",
      "4    -3.604       0.1060  169.881    1               4  Energetic  \n"
     ]
    }
   ],
   "source": [
    "# 2. Load and Explore the Music Data\n",
    "music_data = pd.read_csv('data_moods.csv')\n",
    "print(music_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba74fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define Directory Paths\n",
    "base_dir = 'mood_for_music'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "test_dir = os.path.join(base_dir, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3292de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Image Data Preprocessing\n",
    "# Create an ImageDataGenerator for training and testing\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf0d1a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "# Create generators for training and testing\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b721d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7178 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(48, 48),\n",
    "    batch_size=32,\n",
    "    color_mode='grayscale',\n",
    "    class_mode='categorical'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82e570d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 5. Build the CNN Model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')  # 7 classes for each emotion\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862ab80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa11d116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\home\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1184s\u001b[0m 1s/step - accuracy: 0.2715 - loss: 1.7810 - val_accuracy: 0.4146 - val_loss: 1.5237\n",
      "Epoch 2/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m383s\u001b[0m 423ms/step - accuracy: 0.4144 - loss: 1.5132 - val_accuracy: 0.4737 - val_loss: 1.3790\n",
      "Epoch 3/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 93ms/step - accuracy: 0.4752 - loss: 1.3851 - val_accuracy: 0.5017 - val_loss: 1.3055\n",
      "Epoch 4/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 91ms/step - accuracy: 0.5033 - loss: 1.3083 - val_accuracy: 0.5176 - val_loss: 1.2665\n",
      "Epoch 5/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 93ms/step - accuracy: 0.5211 - loss: 1.2579 - val_accuracy: 0.5219 - val_loss: 1.2436\n",
      "Epoch 6/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 93ms/step - accuracy: 0.5465 - loss: 1.1981 - val_accuracy: 0.5380 - val_loss: 1.2070\n",
      "Epoch 7/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 94ms/step - accuracy: 0.5604 - loss: 1.1669 - val_accuracy: 0.5464 - val_loss: 1.1951\n",
      "Epoch 8/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m780s\u001b[0m 869ms/step - accuracy: 0.5773 - loss: 1.1221 - val_accuracy: 0.5524 - val_loss: 1.1870\n",
      "Epoch 9/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m178s\u001b[0m 197ms/step - accuracy: 0.5849 - loss: 1.0916 - val_accuracy: 0.5574 - val_loss: 1.1828\n",
      "Epoch 10/10\n",
      "\u001b[1m898/898\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m93s\u001b[0m 103ms/step - accuracy: 0.6069 - loss: 1.0327 - val_accuracy: 0.5593 - val_loss: 1.1781\n"
     ]
    }
   ],
   "source": [
    "# 6. Train the Model\n",
    "history = model.fit(train_generator, validation_data=test_generator, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c128123",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# 7. Save the Model\n",
    "model.save('mood_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0202590d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 814ms/step\n",
      "Predicted Mood: happy\n",
      "Recommended Songs:\n",
      "                  name             artist\n",
      "162  Edge of Seventeen       Stevie Nicks\n",
      "587        Tubthumping        Chumbawamba\n",
      "557          The Joker  Steve Miller Band\n",
      "372        Night Moves          Bob Seger\n",
      "502  Spirit In The Sky   Norman Greenbaum\n"
     ]
    }
   ],
   "source": [
    "# 8. Define a Function to Predict Mood and Recommend Music\n",
    "def predict_mood_and_recommend_music(image_path):\n",
    "    # Load and preprocess the image\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    image = cv2.resize(image, (48, 48))\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    image = np.expand_dims(image, axis=-1)\n",
    "    image = image / 255.0  # Normalize the image\n",
    "     # Load the trained model\n",
    "    model = load_model('mood_classification_model.h5')\n",
    "    # Predict the mood\n",
    "    predicted_class = model.predict(image)\n",
    "    predicted_mood = np.argmax(predicted_class)\n",
    "    # Map the predicted mood to the actual labels\n",
    "    mood_labels = {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy', 4: 'neutral', 5: 'sad', 6: 'surprise'}\n",
    "    predicted_mood_label = mood_labels[predicted_mood]\n",
    "    # Recommend music based on the predicted mood\n",
    "    recommendations = music_data[music_data['mood'].str.lower() == predicted_mood_label]\n",
    "    recommended_songs = recommendations[['name', 'artist']].sample(5)\n",
    "    \n",
    "    return predicted_mood_label, recommended_songs\n",
    "# 9. Example Usage\n",
    "image_path = \"C:\\\\Users\\\\home\\\\Desktop\\\\Untitled Folder\\\\mood_for_music\\\\test\\\\happy\\\\PrivateTest_95094.jpg\"  # Replace with an actual image path\n",
    "mood, songs = predict_mood_and_recommend_music(image_path)\n",
    "print(f\"Predicted Mood: {mood}\")\n",
    "print(\"Recommended Songs:\")\n",
    "print(songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80681983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
